{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Firearm-related action recognition with Transformers\n"
      ],
      "metadata": {
        "id": "rsXphuJnwGak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installazione dei pacchetti necessari"
      ],
      "metadata": {
        "id": "spZjupcqwaWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5XUp6LhLgDT"
      },
      "outputs": [],
      "source": [
        "!pip install av\n",
        "!pip install git+https://github.com/Atze00/MoViNet-pytorch.git\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "!pip install gdown==4.6\n",
        "!pip install codecarbon"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import delle librerie utilizzate"
      ],
      "metadata": {
        "id": "cTgnHvvDwjMX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_kFUbVLDJb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torch\n",
        "from movinets import MoViNet\n",
        "from movinets.config import _C\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from tensorflow_docs.vis import embed\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import random\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from codecarbon import EmissionsTracker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scelta dei parametri:\n",
        "\n",
        "```\n",
        "IMPORTA_ELABORAZIONI = True\n",
        "```\n",
        "Questa scelta consente di scaricare le feature map dei vari frame, andando a risparmiare qualche minuto.\n",
        "```\n",
        "ADDESTRA = False\n",
        "```\n",
        "Questa scelta consente di scaricare i pesi della rete, in maniera tale da non dover ripetere l'addestramento.\n",
        "```\n",
        "SALVA_PESI = False\n",
        "```\n",
        "Questa scelta, se settata a True, ci consente di salvare sul drive i pesi della rete addestrata e le elaborazioni dei frame.\n",
        "```\n",
        "MAX_SEQ_LENGTH = 20\n",
        "```\n",
        "Indica il numero di frame scelti per ogni video.\n",
        "```\n",
        "NUM_FEATURES = 2048\n",
        "```\n",
        "Indica il numero delle features estratte.\n",
        "```\n",
        "IMG_SIZE = 224\n",
        "```\n",
        "Indica la dimensione dei frame.\n",
        "```\n",
        "EPOCHS = 30\n",
        "```\n",
        "Indica il numero di epoche\n",
        "```\n",
        "random.seed(42)\n",
        "```\n",
        "Qua viene impostato un seed per la libreria random.\n",
        "```\n",
        "RISK_CLASSIFIER = True\n",
        "```\n",
        "Questo valore, se impostato a True, avvierà il modello di classificazione dei rischi; mentre, se impostato a False, avvirà il modello di gun_classifier."
      ],
      "metadata": {
        "id": "qW3E6Az9xGrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMPORTA_ELABORAZIONI = True #Risparmio tempo importando i file .npy\n",
        "ADDESTRA = False\n",
        "SALVA_PESI = False\n",
        "MAX_SEQ_LENGTH = 20 #Frame da campionare: ne verrà preso uno al centro del video e gli altri in maniera equa a destra e sinistra\n",
        "NUM_FEATURES = 2048\n",
        "IMG_SIZE = 224 #Grandezza immagini\n",
        "EPOCHS = 30\n",
        "random.seed(42)\n",
        "RISK_CLASSIFIER = True"
      ],
      "metadata": {
        "id": "BKM_OASuwS8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SALVA_PESI:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q12u4VqtBCKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RISK_CLASSIFIER:\n",
        "  !gdown \"1Q7P8p9nJkSkABHLDYxpv1hxBWCJK8Bbj\" -O dataset.zip #dataset rischio\n",
        "else:\n",
        "  !gdown \"1CdEx22PRxaJ0V7gaZqvlpt9g39ty04n1\" -O dataset.zip #dataset gun classifier\n",
        "'''\n",
        "risk classifier dataset:\n",
        "\"No_risk\": 0,\n",
        "\"Medium_risk\": 1,\n",
        "\"High_risk\": 2\n",
        "\n",
        "gun classifier dataset:\n",
        "\"No_Gun\": 0,\n",
        "\"Handgun\": 1,\n",
        "\"Machine_Gun\": 2\n",
        "'''\n",
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "nTky3ft0P1MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Di seguto viene inizializzato il dataset, splittando in 70/15/15 le varie parti."
      ],
      "metadata": {
        "id": "nMNdxsMTytKE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRFpBcsMEp1z"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('dataset/class.csv')\n",
        "X = df['filename']\n",
        "y = df['tag']\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle = True, stratify = y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle = True, stratify = y_temp)\n",
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "print(len(X_val))\n",
        "print(len(y_val))\n",
        "print(len(X_test))\n",
        "print(len(y_test))\n",
        "df_train = pd.DataFrame({\n",
        "    'filename': X_train,\n",
        "    'tag': y_train\n",
        "})\n",
        "df_train.to_csv('dataset/train.csv', index=False)\n",
        "\n",
        "df_val = pd.DataFrame({\n",
        "    'filename': X_val,\n",
        "    'tag': y_val\n",
        "})\n",
        "df_val.to_csv('dataset/val.csv', index=False)\n",
        "\n",
        "df_test = pd.DataFrame({\n",
        "    'filename': X_test,\n",
        "    'tag': y_test\n",
        "})\n",
        "df_test.to_csv('dataset/test.csv', index=False)\n",
        "# Percorsi dei file CSV\n",
        "train_csv_path = 'dataset/train.csv'\n",
        "test_csv_path = 'dataset/test.csv'\n",
        "val_csv_path = 'dataset/val.csv'\n",
        "# Cartella di origine dove sono memorizzati i video\n",
        "source_folder = 'dataset'\n",
        "\n",
        "def move_videos(csv_path, destination_folder):\n",
        "    # Leggi il file CSV\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # Itera su ogni riga del DataFrame\n",
        "    for filename in df['filename']:\n",
        "        # Costruisci il percorso completo del file video\n",
        "        video_path = os.path.join(source_folder, filename)\n",
        "        # Verifica se il file esiste\n",
        "        if os.path.exists(video_path):\n",
        "            # Costruisci il percorso di destinazione\n",
        "            destination_path = os.path.join(destination_folder, filename)\n",
        "            # Sposta il file\n",
        "            shutil.move(video_path, destination_path)\n",
        "            print(f\"Spostato {filename} a {destination_folder}\")\n",
        "        else:\n",
        "            print(f\"Il file {filename} non esiste in {source_folder}\")\n",
        "\n",
        "# Processa i file specificati nei CSV\n",
        "move_videos(train_csv_path, 'dataset/train')\n",
        "move_videos(test_csv_path,'dataset/test')\n",
        "move_videos(val_csv_path, 'dataset/val')\n",
        "shutil.move(train_csv_path, 'dataset/train')\n",
        "shutil.move(test_csv_path, 'dataset/test')\n",
        "shutil.move(val_csv_path, 'dataset/val')\n",
        "os.remove('dataset/class.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inizializzo il modello di MoViNet, utilizzato per estrarre le features.\n",
        "Rimuovo il classificatore."
      ],
      "metadata": {
        "id": "v1GeL5Fr4RIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MoViNet(_C.MODEL.MoViNetA0, causal = False, pretrained = True )\n",
        "print(\"Original model head:\", model.classifier)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Conv3d(480, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1)),  # Mantenere l'ultimo layer convoluzionale\n",
        "    nn.Identity()  # Aggiungi un layer di identità\n",
        ")\n",
        "print(\"New model head:\", model.classifier)\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 16, 224, 224)  # (batch_size, channels, frames, height, width)\n",
        "output = model(dummy_input)\n",
        "\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "id": "c_izV8p4eM69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco le funzioni utili per l'estrazione"
      ],
      "metadata": {
        "id": "UsABFS2h4kWL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MP2X5zgEp11"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"dataset/train/train.csv\")\n",
        "val_df = pd.read_csv(\"dataset/val/val.csv\")\n",
        "test_df = pd.read_csv(\"dataset/test/test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for validation: {len(val_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame\n",
        "\n",
        "# estrazione dei frame dal video\n",
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224)):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  if video_length > 300:\n",
        "    frame_step = 15\n",
        "  else:\n",
        "    frame_step = int(video_length/n_frames) - 1\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result\n",
        "\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"].astype(str)), mask_token=None\n",
        ")\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"filename\"].values.tolist()\n",
        "    totale = len(video_paths)\n",
        "    labels = df[\"tag\"].astype(str).values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        progressivo = idx + 1\n",
        "        print(f\"{progressivo}/{totale}: {path}\")\n",
        "        # Create a directory for the current video frames if it doesn't exist\n",
        "\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = frames_from_video_file(os.path.join(root_dir, path), MAX_SEQ_LENGTH, (IMG_SIZE, IMG_SIZE))\n",
        "         # Risolvi la dimensione del batch e dimensione dei canali\n",
        "        frames = np.transpose(frames, (3, 0, 1, 2))  # Da (frames, height, width, channels) a (channels, frames, height, width)\n",
        "        frames = frames[None, ...]  # Aggiungi la dimensione del batch: (1, channels, frames, height, width)\n",
        "        frames = torch.tensor(frames, dtype=torch.float32)  # Converti in tensor\n",
        "\n",
        "        # Estrai le feature dai frame del video\n",
        "        temp_frame_features = np.zeros((MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "        with torch.no_grad():\n",
        "            for i in range(min(MAX_SEQ_LENGTH, frames.shape[2])):\n",
        "                frame = frames[:, :, i, :, :].unsqueeze(2)  # Prendi un singolo frame mantenendo le dimensioni\n",
        "                features = model(frame)\n",
        "                temp_frame_features[i] = features.squeeze().numpy()\n",
        "\n",
        "        frame_features[idx] = temp_frame_features\n",
        "\n",
        "\n",
        "    return frame_features, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seconda della mia scelta, estraggo i frame e ne carico i dati sul drive oppure scarico i file già pronti.\n",
        "Siccome per la feature_extraction utilizzo una rete neurale, tengo conto dei consumi effettuati."
      ],
      "metadata": {
        "id": "R8gqF8zH4qcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if IMPORTA_ELABORAZIONI:\n",
        "  if RISK_CLASSIFIER:\n",
        "    !gdown \"1oR0M8H259uHW_lAUq9pE22LajOpQvhHv\" -O \"transformers.zip\"\n",
        "  else:\n",
        "    !gdown \"1xyBO0CRA4myRcTPQs_tZBMlX729mucG3\" -O \"transformers.zip\"\n",
        "  !unzip transformers.zip\n",
        "  train_data = np.load(\"train_data.npy\")\n",
        "  train_labels = np.load(\"train_labels.npy\")\n",
        "  val_data = np.load(\"val_data.npy\")\n",
        "  val_labels = np.load(\"val_labels.npy\")\n",
        "  test_data = np.load(\"test_data.npy\")\n",
        "  test_labels = np.load(\"test_labels.npy\")\n",
        "else:\n",
        "  tracker = EmissionsTracker()\n",
        "  tracker.start()\n",
        "  train_data, train_labels = prepare_all_videos(train_df, \"dataset/train\")\n",
        "  test_data, test_labels = prepare_all_videos(test_df, \"dataset/test\")\n",
        "  val_data, val_labels = prepare_all_videos(val_df, \"dataset/val\")\n",
        "  emission: float = tracker.stop()\n",
        "  print(f\"Emission: {emission}\")\n",
        "  if SALVA_PESI:\n",
        "    np.save(\"train_data.npy\", train_data)\n",
        "    np.save(\"train_labels.npy\", train_labels)\n",
        "    np.save(\"val_data.npy\", val_data)\n",
        "    np.save(\"val_labels.npy\", val_labels)\n",
        "    np.save(\"test_data.npy\", test_data)\n",
        "    np.save(\"test_labels.npy\", test_labels)\n",
        "    !zip -r transformersElabDS2.zip train_data.npy train_labels.npy test_data.npy test_labels.npy val_data.npy val_labels.npy\n",
        "    shutil.copy(\"transformersElabDS2.zip\", \"/content/drive/My Drive/transformers/transformersElabDS2.zip\")"
      ],
      "metadata": {
        "id": "YhRAb2Doyew-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe PositionalEmbedding crea un livello che aggiunge informazioni posizionali agli input. Questo è fondamentale per i Transformer perché, diversamente dalle reti neurali ricorrenti (RNN), i Transformer non hanno una nozione intrinseca dell'ordine degli elementi nella sequenza. L'aggiunta degli embedding posizionali permette al modello di tener conto della posizione dei frame nel video, migliorando la capacità del modello di apprendere le dipendenze temporali."
      ],
      "metadata": {
        "id": "bcf-6RgY5Pw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfE3-B4dEp13"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.position_embeddings.build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
        "        inputs = tf.cast(inputs, self.compute_dtype)\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe TransformerEncoder rappresenta un singolo strato di codifica dell'architettura Transformer. Questo strato combina meccanismi di attenzione multi-testa con una rete neurale feed-forward, supportata da tecniche di normalizzazione e connessioni residuali per facilitare l'apprendimento di rappresentazioni ricche e contestuali. Ecco una descrizione dettagliata delle componenti e delle operazioni eseguite da questa classe.\n",
        "Il meccanismo di attenzione multi-testa permette al modello di focalizzarsi su diverse parti dell'input simultaneamente e di catturare vari tipi di relazioni e dipendenze a lungo raggio."
      ],
      "metadata": {
        "id": "p3o9DnYs5SqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5jNLvLqEp13"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=keras.activations.gelu),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questo codice costruisce un modello di classificazione video utilizzando un Transformer Encoder per elaborare sequenze di frame. Il modello incorpora informazioni posizionali, cattura dipendenze temporali tra i frame, riduce la dimensionalità con global max pooling, e infine usa un layer denso con attivazione softmax per produrre le probabilità delle classi. Il modello è poi compilato con l'ottimizzatore Adam e una funzione di perdita adatta per classificazione multi-classe."
      ],
      "metadata": {
        "id": "IvD5IDYL5ggz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12qZus7EEp13"
      },
      "outputs": [],
      "source": [
        "def get_compiled_model(shape):\n",
        "    sequence_length = MAX_SEQ_LENGTH\n",
        "    embed_dim = NUM_FEATURES\n",
        "    dense_dim = 4\n",
        "    num_heads = 1\n",
        "    classes = len(label_processor.get_vocabulary())\n",
        "    inputs = keras.Input(shape=shape)\n",
        "    x = PositionalEmbedding(\n",
        "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
        "    )(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_experiment():\n",
        "    filepath = \"/tmp/video_classifier.weights.h5\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "    model = get_compiled_model(train_data.shape[1:])\n",
        "    if ADDESTRA:\n",
        "      tracker = EmissionsTracker()\n",
        "      tracker.start()\n",
        "      history = model.fit(\n",
        "        train_data,\n",
        "        train_labels,\n",
        "        validation_data=(val_data, val_labels),\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "        batch_size = 16,\n",
        "    )\n",
        "      emission: float = tracker.stop()\n",
        "      print(f\"Emission: {emission}\")\n",
        "      if SALVA_PESI: shutil.copy(filepath, \"/content/drive/My Drive/transformers/video_gun_classifier.weights.h5\")\n",
        "    else:\n",
        "      if RISK_CLASSIFIER:\n",
        "        !gdown \"1-E7XEIQkobqxHxnZKa7J4HC4ob_1Zdc1\" -O \"video_classifier.weights.h5\"\n",
        "      else:\n",
        "        !gdown \"1mfKophETzw_10_ByPClCDfaviXVZ701t\" -O \"video_classifier.weights.h5\"\n",
        "      model.load_weights(\"video_classifier.weights.h5\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = run_experiment()"
      ],
      "metadata": {
        "id": "YUtqL5FS7C_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valuto il modello andando a stampare le metriche più importanti."
      ],
      "metadata": {
        "id": "nh9rYPae67kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_data, test_labels):\n",
        "    # Predict the labels for the test set\n",
        "    predictions = model.predict(test_data)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "    print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(test_labels, predicted_labels))\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rqjGNM03kUNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(trained_model, test_data, test_labels)"
      ],
      "metadata": {
        "id": "-I-yY2G7kkV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}